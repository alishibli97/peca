{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/learn/intro-to-machine-learning\n",
    "# https://campus.datacamp.com/courses/supervised-learning-with-scikit-learn/classification?ex=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "\n",
    "# >>> from sklearn.datasets import load_iri\n",
    "# >>> from sklearn.linear_model import LogisticRegression\n",
    "# >>> from sklearn.model_selection import RandomizedSearchCV\n",
    "# >>> from scipy.stats import uniform\n",
    "# >>> iris = load_iris()\n",
    "# >>> logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
    "# ...                               random_state=0)\n",
    "# >>> distributions = dict(C=uniform(loc=0, scale=4),\n",
    "# ...                      penalty=['l2', 'l1'])\n",
    "# >>> clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "# >>> search = clf.fit(iris.data, iris.target)\n",
    "# >>> search.best_params_\n",
    "# {'C': 2..., 'penalty': 'l1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "# >>> from sklearn import svm, datasets\n",
    "# >>> from sklearn.model_selection import GridSearchCV\n",
    "# >>> iris = datasets.load_iris()\n",
    "# >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "# >>> svc = svm.SVC()\n",
    "# >>> clf = GridSearchCV(svc, parameters)\n",
    "# >>> clf.fit(iris.data, iris.target)\n",
    "# GridSearchCV(estimator=SVC(),\n",
    "#              param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n",
    "# >>> sorted(clf.cv_results_.keys())\n",
    "# ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
    "#  'param_C', 'param_kernel', 'params',...\n",
    "#  'rank_test_score', 'split0_test_score',...\n",
    "#  'split2_test_score', ...\n",
    "#  'std_fit_time', 'std_score_time', 'std_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provided below are 4 example applications of machine learning. Which of them is a supervised classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Using labeled financial data to predict whether the value of a stock will go up or go down next week.\n",
    "\n",
    "b. Using labeled housing price data to predict the price of a new house based on various features.\n",
    "\n",
    "c. Using unlabeled data to cluster the students of an online education company into different categories based on their learning styles.\n",
    "\n",
    "d. Using labeled financial data to predict what the value of a stock will be next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models:\n",
    "1. Logistic Regression\n",
    "2. NaÃ¯ve Bayes\n",
    "3. Stochastic Gradient Descent\n",
    "4. K-Nearest Neighbours\n",
    "5. Decision Tree\n",
    "6. Random Forest\n",
    "7. Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models definitons - Data splitting - Fitting - Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report - Area under the ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Think About it: Is there overfitting / underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given below are four example applications of machine learning. Your job is to pick the one that is best framed as a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. (correct) An e-commerce company using labeled customer data to predict whether or not a customer will purchase a particular item.\n",
    "\n",
    "b. A healthcare company using data about cancer tumors (such as their geometric measurements) to predict whether a new tumor is benign or malignant.\n",
    "\n",
    "c. A restaurant using review data to ascribe positive or negative sentiment to a given review.\n",
    "\n",
    "d. A bike share company using time and weather data to predict the number of bikes being rented at any given hour.\n",
    "press"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models:\n",
    "\n",
    "- Linear Regression.\n",
    "- SVR\n",
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models definitons - Data splitting - Fitting - Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization I: Lasso\n",
    "In the video, you saw how Lasso selected out the 'RM' feature as being the most important for predicting Boston house prices, while shrinking the coefficients of certain other features to 0. Its ability to perform feature selection in this way becomes even more useful when you are dealing with data involving thousands of features.\n",
    "\n",
    "In this exercise, you will fit a lasso regression to the Gapminder data you have been working with and plot the coefficients. Just as with the Boston data, you will find that the coefficients of some features are shrunk to 0, with only the most important ones remaining.\n",
    "\n",
    "The feature and target variable arrays have been pre-loaded as X and y.\n",
    "\n",
    "Regularization II: Ridge\n",
    "Lasso is great for feature selection, but when building regression models, Ridge regression should be your first choice.\n",
    "\n",
    "Recall that lasso performs regularization by adding to the loss function a penalty term of the absolute value of each coefficient multiplied by some alpha. This is also known as  regularization because the regularization term is the  norm of the coefficients. This is not the only way to regularize, however.\n",
    "\n",
    "If instead you took the sum of the squared values of the coefficients multiplied by some alpha - like in Ridge regression - you would be computing the  norm. In this exercise, you will practice fitting ridge regression models over a range of different alphas, and plot cross-validated  scores for each, using this function that we have defined for you, which plots the  score as well as standard error for each alpha:\n",
    "\n",
    "def display_plot(cv_scores, cv_scores_std):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(alpha_space, cv_scores)\n",
    "\n",
    "    std_error = cv_scores_std / np.sqrt(10)\n",
    "\n",
    "    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n",
    "    ax.set_ylabel('CV Score +/- Std Error')\n",
    "    ax.set_xlabel('Alpha')\n",
    "    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n",
    "    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n",
    "    ax.set_xscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "Don't worry about the specifics of the above function works. The motivation behind this exercise is for you to see how the  score varies with different alphas, and to understand the importance of selecting the right value for alpha. You'll learn how to tune alpha in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and pipelines - putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "# >>> from sklearn.svm import SVC\n",
    "# >>> from sklearn.preprocessing import StandardScaler\n",
    "# >>> from sklearn.datasets import make_classification\n",
    "# >>> from sklearn.model_selection import train_test_split\n",
    "# >>> from sklearn.pipeline import Pipeline\n",
    "# >>> X, y = make_classification(random_state=0)\n",
    "# >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "# ...                                                     random_state=0)\n",
    "# >>> pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
    "# >>> # The pipeline can be used as any other estimator\n",
    "# >>> # and avoids leaking the test set into the train set\n",
    "# >>> pipe.fit(X_train, y_train)\n",
    "# Pipeline(steps=[('scaler', StandardScaler()), ('svc', SVC())])\n",
    "# >>> pipe.score(X_test, y_test)\n",
    "# 0.88"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
